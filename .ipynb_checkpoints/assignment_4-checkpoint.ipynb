{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bh1Bihw1ox6S"
   },
   "source": [
    "**CSOC ML - Assignment 4**\n",
    "\n",
    "Welcome to CSOC ML (2019) - Assignment 4. In this assignment, you will make a model that recognises faces! We will also explore Dimensionality Reduction.\n",
    "\n",
    "In this assignment, we will use NumPy, Matplotlib, sklearn, and OpenCV (an image processing library). Instead of OpenCV, one could instead use other image processing libraries like skimage Pillow, scipy.ndimage etc. But for the sake of uniformity, you have to complete this assignment using OpenCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g18nlt7UpFdm"
   },
   "source": [
    "# **IMPORTING LIBRARIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WwwRk0jko2eJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "import time\n",
    "import matplotlib\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.svm import LinearSVC\n",
    "import cv2\n",
    "from google.colab import drive\n",
    "from sklearn import datasets\n",
    "from numpy.linalg import eig\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vth7FRv3pgNV"
   },
   "source": [
    "#MOUNTING DRIVE\n",
    "\n",
    "This will act like your local storage. Once you save the data in the drive you can access it just like you access the data in the local storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "x01XWBPnpMyy",
    "outputId": "cfcb53cd-091e-4bfc-909a-b4aa86d56cff"
   },
   "outputs": [],
   "source": [
    "drive.mount('/content/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KPZl61P0pVm3",
    "outputId": "4952e97c-ba0e-47af-ca7d-6475621d8c7c"
   },
   "outputs": [],
   "source": [
    "cd '/content/gdrive/My Drive/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KAPQ1zlmp1W6"
   },
   "source": [
    "#PCA\n",
    "\n",
    "You have studied PCA(Principal Component Analysis) previous week. So this time you will implement \n",
    "your own PCA. That will give you more knowlegdge about how PCA works. Dimensionality Reduction plays an important role while dealing with large datasets so before moving to bigger stuff we are going to learn a little about PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8UxRIgKxp9_C"
   },
   "source": [
    "#LOADING THE DATASET\n",
    "\n",
    "We will we working on the iris dataset to see how PCA actually works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GD5xyCMLpYNy"
   },
   "outputs": [],
   "source": [
    "# We are importing iris dataset from the sklearn library\n",
    "#This data sets consists of 3 different types of irisesâ€™ (Setosa, Versicolour, and Virginica) petal and sepal length, stored in a 150x4 numpy.ndarray\n",
    "#The rows being the samples and the columns being: Sepal Length, Sepal Width, Petal Length and Petal Width.\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data #features\n",
    "y = iris.target #types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nm9ejf-qqRYJ"
   },
   "source": [
    "#IMPLEMENTING PCA\n",
    "\n",
    "**The real stuff....**\n",
    "\n",
    "Now you are going to implement PCA. Just follow the steps in the comment section of the code. Read the code carefully. In case of any help you can refer to this link\n",
    "[https://machinelearningmastery.com/calculate-principal-component-analysis-scratch-python/](https://machinelearningmastery.com/calculate-principal-component-analysis-scratch-python/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YrX7thtOqQcK"
   },
   "outputs": [],
   "source": [
    "#YOUR CODE STARTS HERE\n",
    "# First you have to calculate the mean of each column\n",
    "\n",
    "M = \n",
    "\n",
    "# Now centre columns by subtracting the means from the data\n",
    "\n",
    "C = \n",
    "\n",
    "#Get the covariance matrix using numpy\n",
    "\n",
    "V = \n",
    "\n",
    "#Now do eigendecomposition of covariance matrix. Don't worry there is also a library in numpy for that. Check numpy.linalg.eig\n",
    "#Store the eigenvalues and eigenvectors in eig_val and eig_vec respectively\n",
    "\n",
    "eig_val, eig_vec = \n",
    "\n",
    "\n",
    "#Print your eigen vectors and eigen values\n",
    "for i in range(len(eig_val)):\n",
    "    eigvec = eig_vec[:,i].reshape(1,4).T\n",
    "\n",
    "    print('Eigenvector {}: \\n{}'.format(i+1, eigvec))\n",
    "    print('Eigenvalue {} from covariance matrix: {}'.format(i+1, eig_val[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I7LxCEcTqwXk"
   },
   "outputs": [],
   "source": [
    "#YOUR CODE STARTS HERE\n",
    "\n",
    "# Make the (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = \n",
    "\n",
    "# Sort the tuples (eig_pairs) from high to low\n",
    "\n",
    "\n",
    "#YOUR CODE ENDS HERE\n",
    "\n",
    "# Visually confirm that the list is correctly sorted by decreasing eigenvalues\n",
    "# The values printed must be in decreasing order\n",
    "for i in eig_pairs:\n",
    "    print(i[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PBzP9OM0rayf"
   },
   "outputs": [],
   "source": [
    "#Now the take the first k eigen_vectors. Here we have taken k as 2.\n",
    "weights = np.hstack((eig_pairs[0][1].reshape(4,1), eig_pairs[1][1].reshape(4,1)))\n",
    "print('Matrix of Weight :\\n', weights)\n",
    "\n",
    "#Now multiply the matrix X with the weights to get the reduced matrix and name the matrix as reduced_x\n",
    "#YOUR CODE STARTS HERE\n",
    "\n",
    "reduced_x = \n",
    "\n",
    "#YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QYKMgEbYrsg7"
   },
   "source": [
    "#VISUALISING AND COMPARING\n",
    "\n",
    "Now we will visualise and compare the performance of our pca with the inbuilt pca provided to us by sklearn library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8LpPvwGzrp5A"
   },
   "outputs": [],
   "source": [
    "#Applying Sklearn PCA.\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "x_pca=pca.transform(X)\n",
    "\n",
    "#Visually comparing the plots after applying our own pca and sklearn pca function\n",
    "plt.subplot(221)\n",
    "plt.scatter(x_pca[:,0],x_pca[:,1],c=y)\n",
    "plt.title('Sklearn PCA')\n",
    "plt.subplot(222)\n",
    "plt.scatter(reduced_x[:,0],reduced_x[:,1],c=y)\n",
    "plt.title('PCA implemented by you')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PNUbiDKWr_0y"
   },
   "source": [
    "We can see from the above figure that the main difference between the two graphs is that our graph is shifted and also our PCA is reflecting mirror image about horizontal axis but we can see the distance between any two points is same in our PCA as well as in PCA of sklearn i.e Information is preserved in both the graphs. We can also see this by comparing the performances of both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "89yU8wrrr0uZ"
   },
   "outputs": [],
   "source": [
    "#Accuracies on both the reduced training sets\n",
    "\n",
    "#Fitting the SVC model on sklearn PCA dataset\n",
    "clf = svm.SVC(gamma='auto')\n",
    "clf.fit(x_pca, y)\n",
    "y_pred=clf.predict(x_pca)\n",
    "print('Accuracy on the sklearn reduced dataset: ',sum(y==y_pred)/len(y_pred))\n",
    "\n",
    "#Fitting the SVC model on our PCA dataset\n",
    "clf1 = svm.SVC(gamma='auto')\n",
    "clf1.fit(reduced_x,y)\n",
    "y_pred=clf1.predict(reduced_x)\n",
    "print('Accuracy on the our PCA reduced dataset: ',sum(y==y_pred)/len(y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cbTWkNNesQIX"
   },
   "source": [
    "We see that acuuracies are the same!! So the pca implemented by you is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4S6iThyPsVnO"
   },
   "source": [
    "**CHEER UP!!!!!** \n",
    "\n",
    "You are halfway through the assignment.\n",
    "Now comes the major part of assignment.Now we will make a face recogniser model.\n",
    "But first there are some things you have to do :-\n",
    "\n",
    "\n",
    "*   First you have to download the dataset provided in the link.\n",
    "\n",
    "      LINK TO DATASET :  https://drive.google.com/drive/folders/1sXCH0jljR35xrL4c1d04A_Ra4V6Bvey6?usp=sharing\n",
    "*   Second you have to upload the dataset on the drive which you have mounted (your google drive).\n",
    "\n",
    "**Note**- Don't change the name of the folder i.e all the data must be stored in the data folder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oqK5BgI0s_no"
   },
   "source": [
    "#Loading data\n",
    "\n",
    "**Task 1**\n",
    "\n",
    "The first task is to load the images from data/folder as NumPy array. First manually go through the data, and understand how it is organised.\n",
    "\n",
    "X should be a NumPy array containing images and y should contain the corresponding labels.\n",
    "\n",
    "Go through os.path.join and os.listdir functions. Refer to cv2.imread function for loading image. Note the images are colored so you have to store them as black and white."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T6lHb5r0sO38"
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for sub_dir in os.listdir('data'):\n",
    "    if not os.path.isdir(os.path.join('data', sub_dir)):\n",
    "        continue\n",
    "\n",
    "    label = int(sub_dir)\n",
    "    for file in os.listdir(os.path.join('data', sub_dir)):\n",
    "        #YOUR CODE STARTS HERE\n",
    "\n",
    "        # get the full path of the image. (Hint: use os library as used above)\n",
    "        filename = \n",
    "        # Read the image using opencv\n",
    "        image=\n",
    "\n",
    "        # Append the image and label in the X and y list respectively\n",
    "\n",
    "        #YOUR CODE ENDS HERE\n",
    "        \n",
    "X = np.array(X, dtype='float64')\n",
    "y = np.array(y)\n",
    "\n",
    "print(\"X.shape: {}, y.shape: {}\".format(X.shape, y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "It9xlCRruFfu"
   },
   "source": [
    "#Analysing data\n",
    "\n",
    "Let us first visualise the first image of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OFgjutn-sLPq"
   },
   "outputs": [],
   "source": [
    "imgplot = plt.imshow(X[1],cmap='gray')\n",
    "plt.show()\n",
    "print('Number of Images: ',X.shape[0])\n",
    "print('Number of features for each image: ',X.shape[1]*X.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ZFYINDTuQei"
   },
   "source": [
    "#Feature Extraction\n",
    "**Task 2**\n",
    "\n",
    "Currently, X is a 3D NumPy array. For each of the 400 images, we have a corresponding 2D matrix storing the pixel values. We will flatten this 2D matrix and use the flattened vector as the feature vector for the image.\n",
    "\n",
    "Note that the number of features obtained this way are very large. There are a number of better methods to extract features from images like convolutional neural networks but for now lets go with this.\n",
    "\n",
    "Reshape X into 400xN, where N is the number of features (200x180)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8RJMz9MzuJ6N"
   },
   "outputs": [],
   "source": [
    "#YOUR CODE STARTS HERE\n",
    "X = \n",
    "#YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6nwC_fvSub8A"
   },
   "source": [
    "#Dimensionality Reduction\n",
    "\n",
    "First, we will use Dimensionality Reduction for Data Visualisation. We will reduce the dimensionality of the data to 2 and plot it. Before applying PCA (one of the Dimensionality Reduction algorithms), we will standardise the data. For sake of clarity, we will be only using the data corresponding to the first num_labels classes. Later you can experiment by changing this parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SgYvLEcUujLC"
   },
   "outputs": [],
   "source": [
    "num_labels = 5\n",
    "X_sample = X[y <= num_labels]\n",
    "y_sample = y[y <= num_labels]\n",
    "\n",
    "X_sample_scaled = normalize(X_sample, norm='l2', axis=0, copy=True, return_norm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YoLB-cXdvH_b"
   },
   "source": [
    "**Task 3**\n",
    "\n",
    "Reduce the dimensionality of X_sample_scaled to 2 using sklearn.decomposition.PCA and assign it to X_sample_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_5WsHbZivFXY"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE STARTS HERE\n",
    "\n",
    "\n",
    "# YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YGOmXnvQvOWz"
   },
   "outputs": [],
   "source": [
    "#Visualising the data\n",
    "num_labels=5\n",
    "plt.scatter(X_sample_2d[:,0],X_sample_2d[:,1],c=y_sample,cmap=plt.cm.get_cmap('Accent', num_labels))\n",
    "\n",
    "color = plt.colorbar()\n",
    "loc = np.arange(1, num_labels+1)\n",
    "color=color.set_ticks(loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SOYNnHHpvXbM"
   },
   "source": [
    "\n",
    "We can clearly see PCA has projected the highly dimensional dataset into 2 dimensions, which we can see on plotting look clustered.\n",
    "\n",
    "Although this data visualisation is not very useful for supervised learning tasks, they prove useful for unsupervised learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rTQzSHCuwhna"
   },
   "source": [
    "#Speeding up training and reducing memory\n",
    "The other two applications of Dimensionality reduction are, speeding up training and reducing memory required to store the data. We will start by investigating the memory required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "syi6lBRswgCj"
   },
   "outputs": [],
   "source": [
    "print(\"Memory used by X_train: {:.2f}MB\".format(X.nbytes / 1024 / 1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GE4UQ-jhwqFC"
   },
   "source": [
    "In this case, because of low resolution of images and less data, the memory used in storing data is already quite low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TvnlwEI-wunr"
   },
   "source": [
    "#Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JSiQGxtovZ6z"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=34)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GZ6OQvcNw_bX"
   },
   "source": [
    "#Standardising the Dataset\n",
    "Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data.\n",
    "\n",
    "Standardising the dataset is not only helpful for training machine learning models but is also a requirement before applying PCA. We will standardise all three X_train, X_val and X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d2wIPDSkxA6E"
   },
   "outputs": [],
   "source": [
    "# Standardize the data (Hint: See above)\n",
    "# YOUR CODE STARTS HERE\n",
    "\n",
    "X_train_scaled = \n",
    "X_val_scaled = \n",
    "\n",
    "# YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hcNls6OTxath"
   },
   "source": [
    "#Training and Hyperparameter Tuning\n",
    "**Without Dimensionality reduction**\n",
    "\n",
    "We will be using time magic command in Jupyter Notebook to measure the time taken in fitting the model. Read up about other useful magic commands like timeit prun, lprun on the web page linked before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GVMw3qHKxXhi"
   },
   "outputs": [],
   "source": [
    "clf = LinearSVC(C=.001)\n",
    "%time clf = clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Training accuracy: {:.4f}, Val Accuracy: {:.4f}\".format(clf.score(X_train_scaled, y_train), clf.score(X_val_scaled, y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wH9PmdRpxsaB"
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=.001)\n",
    "%time clf = clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Training accuracy: {:.4f}, Val Accuracy: {:.4f}\".format(clf.score(X_train_scaled, y_train), clf.score(X_val_scaled, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "La1j9tzhx1XG"
   },
   "source": [
    "**With Dimensionality reduction**\n",
    "\n",
    "Clearly, the time taken to train the model is considerably high. It is infeasible to perform hyperparameter tuning when training the model takes so much time. In some cases when the dataset is even bigger, we might not be able to even train the model with the computing resources in hand. PCA becomes essential to speed up training.\n",
    "\n",
    "#Task 4\n",
    "\n",
    "Make an object of PCA class named pca so that 99% of variance is preserved. Refer to the documentation of PCA class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "df5DyAUKxz1i"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE STARTS HERE\n",
    "\n",
    "\n",
    "    \n",
    "# YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nbfurqM6xu8F"
   },
   "outputs": [],
   "source": [
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_val_pca = pca.transform(X_val_scaled)\n",
    "\n",
    "print(\"Dimensionality reduced to:\", pca.n_components_)\n",
    "\n",
    "clf = LogisticRegression(C=10000,multi_class='auto')\n",
    "%time clf = clf.fit(X_train_pca, y_train)\n",
    "\n",
    "print(\"Training accuracy: {:.4f}, Val Accuracy: {:.4f}\".format(clf.score(X_train_pca, y_train), clf.score(X_val_pca, y_val)))\n",
    "\n",
    "clf = LinearSVC(C=1)\n",
    "%time clf = clf.fit(X_train_pca, y_train)\n",
    "\n",
    "print(\"Training accuracy: {:.4f}, Val Accuracy: {:.4f}\".format(clf.score(X_train_pca, y_train), clf.score(X_val_pca, y_val)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ppyGMRYUyGRb"
   },
   "source": [
    "Clealy you can see that the training time is reduced as compared to that without pca. Although PCA is faster at the cost of accuracy the models can be fine tuned easily and more experiments can be performed. This is a huge benifit of PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_OYAH5WlyxY3"
   },
   "source": [
    "#APPLY ANOTHER ALGORITHM\n",
    "\n",
    "Play with the dataset. Try to apply algorithm like SVC, decision trees etc. and check whether you are getting higher accuracies using these algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Eo3-4DOyvSs"
   },
   "outputs": [],
   "source": [
    "#YOUR CODE STARTS HERE\n",
    "\n",
    "\n",
    "#YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TLeZXZQ6zAjp"
   },
   "source": [
    "#Testing\n",
    "We now test our model on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k4CIjRlUzCJY"
   },
   "outputs": [],
   "source": [
    "def plot_gallery(images, titles, rows=3, cols=4):\n",
    "    plt.figure()\n",
    "    for i in range(rows * cols):\n",
    "        fig = plt.subplot(rows, cols, i + 1)\n",
    "        plt.imshow(images[i], cmap=plt.cm.gray)\n",
    "        plt.title(titles[i])\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "        \n",
    "def titles(y_pred, y_test):\n",
    "    for i in range(y_pred.shape[0]):\n",
    "        yield 'predicted: {0}\\ntrue: {1}'.format(y_pred[i], y_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dz5LWHsczQrn"
   },
   "outputs": [],
   "source": [
    "X_test_scaled = normalize(X_test, norm='l2', axis=0, copy=True, return_norm=False)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "y_pred = clf.predict(X_test_pca)\n",
    "print('Accuracy score: {:.4f}'.format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5DuWxhWdzVhH"
   },
   "source": [
    "#PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DB4DBU3mzW1t"
   },
   "outputs": [],
   "source": [
    "prediction_titles = list(titles(y_pred, y_test))\n",
    "plot_gallery(X_test.reshape(-1, 200, 180), prediction_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XszEQIfEzhC0"
   },
   "source": [
    "Congratulations you have built a face recogniser! You also explored the application of dimensionality reduction in this assignment.\n",
    "\n",
    "If you have sincerely completed the material and the assignments of last two sections, you have gained sufficient knowledge to solve real life supervised Machine Learning problems. You can now go explore the world of competitive machine learning on Kaggle.com."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment 4",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
